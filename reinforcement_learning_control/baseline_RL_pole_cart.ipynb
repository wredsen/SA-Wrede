{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import A2C, DQN\n",
    "\n",
    "# using wredsen's symbtools fork (https://github.com/wredsen/symbtools @ DAE_statefeedback), assuming repos SA-Wrede and symbtools share the same parent directory\n",
    "sys.path.append('../../symbtools/')\n",
    "import symbtools as st\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions used for classic control environments.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional, SupportsFloat, Tuple\n",
    "\n",
    "\n",
    "def verify_number_and_cast(x: SupportsFloat) -> float:\n",
    "    \"\"\"Verify parameter is a single number and cast to a float.\"\"\"\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        raise ValueError(f\"An option ({x}) could not be converted to a float.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def maybe_parse_reset_bounds(\n",
    "    options: Optional[dict], default_low: float, default_high: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    This function can be called during a reset() to customize the sampling\n",
    "    ranges for setting the initial state distributions.\n",
    "\n",
    "    Args:\n",
    "      options: Options passed in to reset().\n",
    "      default_low: Default lower limit to use, if none specified in options.\n",
    "      default_high: Default upper limit to use, if none specified in options.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of the lower and upper limits.\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        return default_low, default_high\n",
    "\n",
    "    low = options.get(\"low\") if \"low\" in options else default_low\n",
    "    high = options.get(\"high\") if \"high\" in options else default_high\n",
    "\n",
    "    # We expect only numerical inputs.\n",
    "    low = verify_number_and_cast(low)\n",
    "    high = verify_number_and_cast(high)\n",
    "    if low > high:\n",
    "        raise ValueError(\n",
    "            f\"Lower bound ({low}) must be lower than higher bound ({high}).\"\n",
    "        )\n",
    "\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A utility class to collect render frames from a function that computes a single frame.\"\"\"\n",
    "from typing import Any, Callable, List, Optional, Set\n",
    "\n",
    "# list of modes with which render function returns None\n",
    "NO_RETURNS_RENDER = {\"human\"}\n",
    "\n",
    "# list of modes with which render returns just a single frame of the current state\n",
    "SINGLE_RENDER = {\"single_rgb_array\", \"single_depth_array\", \"single_state_pixels\"}\n",
    "\n",
    "\n",
    "class Renderer:\n",
    "    \"\"\"This class serves to easily integrate collection of renders for environments that can computes a single render.\n",
    "\n",
    "    To use this function:\n",
    "    - instantiate this class with the mode and the function that computes a single frame\n",
    "    - call render_step method each time the frame should be saved in the list\n",
    "      (usually at the end of the step and reset methods)\n",
    "    - call get_renders whenever you want to retrieve renders\n",
    "      (usually in the render method)\n",
    "    - call reset to clean the render list\n",
    "      (usually in the reset method of the environment)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: Optional[str],\n",
    "        render: Callable[[str], Any],\n",
    "        no_returns_render: Optional[Set[str]] = None,\n",
    "        single_render: Optional[Set[str]] = None,\n",
    "    ):\n",
    "        \"\"\"Instantiates a Renderer object.\n",
    "\n",
    "        Args:\n",
    "            mode (Optional[str]): Way to render\n",
    "            render (Callable[[str], Any]): Function that receives the mode and computes a single frame\n",
    "            no_returns_render (Optional[Set[str]]): Set of render modes that don't return any value.\n",
    "                The default value is the set {\"human\"}.\n",
    "            single_render (Optional[Set[str]]): Set of render modes that should return a single frame.\n",
    "                The default value is the set {\"single_rgb_array\", \"single_depth_array\", \"single_state_pixels\"}.\n",
    "        \"\"\"\n",
    "        if no_returns_render is None:\n",
    "            no_returns_render = NO_RETURNS_RENDER\n",
    "        if single_render is None:\n",
    "            single_render = SINGLE_RENDER\n",
    "\n",
    "        self.no_returns_render = no_returns_render\n",
    "        self.single_render = single_render\n",
    "        self.mode = mode\n",
    "        self.render = render\n",
    "        self.render_list = []\n",
    "\n",
    "    def render_step(self) -> None:\n",
    "        \"\"\"Computes a frame and save it to the render collection list.\n",
    "\n",
    "        This method should be usually called inside environment's step and reset method.\n",
    "        \"\"\"\n",
    "        if self.mode is not None and self.mode not in self.single_render:\n",
    "            render_return = self.render(self.mode)\n",
    "            if self.mode not in self.no_returns_render:\n",
    "                self.render_list.append(render_return)\n",
    "\n",
    "    def get_renders(self) -> Optional[List]:\n",
    "        \"\"\"Pops all the frames from the render collection list.\n",
    "\n",
    "        This method should be usually called in the environment's render method to retrieve the frames collected till this time step.\n",
    "        \"\"\"\n",
    "        if self.mode in self.single_render:\n",
    "            return self.render(self.mode)\n",
    "        elif self.mode is not None and self.mode not in self.no_returns_render:\n",
    "            renders = self.render_list\n",
    "            self.render_list = []\n",
    "            return renders\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the render collection list.\n",
    "\n",
    "        This method should be usually called inside environment's reset method.\n",
    "        \"\"\"\n",
    "        self.render_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym environment for crane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "#from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "#from gym.utils.renderer import Renderer\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ### Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ### Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ### Rewards\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "    ### Starting State\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ### Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('CartPole-v1')\n",
    "    ```\n",
    "\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\", \"single_rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.deltaT = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.renderer = Renderer(self.render_mode, self._render)\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.deltaT * x_dot\n",
    "            x_dot = x_dot + self.deltaT * xacc\n",
    "            theta = theta + self.deltaT * theta_dot\n",
    "            theta_dot = theta_dot + self.deltaT * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.deltaT * xacc\n",
    "            x = x + self.deltaT * x_dot\n",
    "            theta_dot = theta_dot + self.deltaT * thetaacc\n",
    "            theta = theta + self.deltaT * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        self.renderer.render_step()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, {\"info\": False}#False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        #super().reset()(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = np.random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "        self.renderer.reset()\n",
    "        self.renderer.render_step()\n",
    "        return np.array(self.state, dtype=np.float32)#, {}\n",
    "\n",
    "    def render(self):\n",
    "        return self.renderer.get_renders()\n",
    "\n",
    "    def _render(self, mode=\"human\"):\n",
    "        assert mode in self.metadata[\"render_modes\"]\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode in {\"rgb_array\", \"single_rgb_array\"}\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif mode in {\"rgb_array\", \"single_rgb_array\"}:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment without renderings for training\n",
    "env = CartPoleEnv()\n",
    "# environment with renderings for validating\n",
    "env_rendering = CartPoleEnv(render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(env.action_space.sample())\n",
    "env.observation_space\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./a2c_cartpole_tensorboard/A2C_5\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 16.7     |\n",
      "|    ep_rew_mean        | 16.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1437     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.674   |\n",
      "|    explained_variance | 0.67     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.638    |\n",
      "|    value_loss         | 2.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 17.3     |\n",
      "|    ep_rew_mean        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1500     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.686   |\n",
      "|    explained_variance | 0.289    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.75     |\n",
      "|    value_loss         | 5.89     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 18.6     |\n",
      "|    ep_rew_mean        | 18.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1512     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | 0.0686   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -1.38    |\n",
      "|    value_loss         | 50.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 20.4     |\n",
      "|    ep_rew_mean        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1498     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.665   |\n",
      "|    explained_variance | 0.25     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.63     |\n",
      "|    value_loss         | 4.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 22.2     |\n",
      "|    ep_rew_mean        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1499     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | -0.0124  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.51     |\n",
      "|    value_loss         | 5.99     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24.4     |\n",
      "|    ep_rew_mean        | 24.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1508     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | 0.0155   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -17.3    |\n",
      "|    value_loss         | 630      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.7     |\n",
      "|    ep_rew_mean        | 25.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1513     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.688   |\n",
      "|    explained_variance | 0.0367   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -2       |\n",
      "|    value_loss         | 176      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.5     |\n",
      "|    ep_rew_mean        | 25.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1520     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | -0.00771 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.42     |\n",
      "|    value_loss         | 4.76     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 24.9     |\n",
      "|    ep_rew_mean        | 24.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1527     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.692   |\n",
      "|    explained_variance | 0.0119   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.22     |\n",
      "|    value_loss         | 4.18     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.2     |\n",
      "|    ep_rew_mean        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1529     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.665   |\n",
      "|    explained_variance | 3.04e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -21      |\n",
      "|    value_loss         | 1.09e+03 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 26.9     |\n",
      "|    ep_rew_mean        | 26.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1533     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.534   |\n",
      "|    explained_variance | -0.00977 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.12     |\n",
      "|    value_loss         | 3.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.2     |\n",
      "|    ep_rew_mean        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1538     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 0.00784  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.31     |\n",
      "|    value_loss         | 3.08     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 32.2      |\n",
      "|    ep_rew_mean        | 32.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1541      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.646    |\n",
      "|    explained_variance | -1.54e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.873     |\n",
      "|    value_loss         | 2.63      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 36.4     |\n",
      "|    ep_rew_mean        | 36.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1544     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.564   |\n",
      "|    explained_variance | 0.000895 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.93     |\n",
      "|    value_loss         | 2.23     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 38.4     |\n",
      "|    ep_rew_mean        | 38.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1545     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.651   |\n",
      "|    explained_variance | 0.0106   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.646    |\n",
      "|    value_loss         | 1.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 42.2     |\n",
      "|    ep_rew_mean        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1546     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.669   |\n",
      "|    explained_variance | 0.00112  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.601    |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 46.1     |\n",
      "|    ep_rew_mean        | 46.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1549     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.655   |\n",
      "|    explained_variance | 9.55e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.558    |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 50.3     |\n",
      "|    ep_rew_mean        | 50.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 1551     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.668   |\n",
      "|    explained_variance | 0.000251 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.475    |\n",
      "|    value_loss         | 0.933    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 55.7      |\n",
      "|    ep_rew_mean        | 55.7      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1551      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.624    |\n",
      "|    explained_variance | -4.94e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.295     |\n",
      "|    value_loss         | 0.677     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 59.4      |\n",
      "|    ep_rew_mean        | 59.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 1550      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.63     |\n",
      "|    explained_variance | -0.000351 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.313     |\n",
      "|    value_loss         | 0.463     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f1654534970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning!\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_rendering = CartPoleEnv(render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "obs = env_rendering.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, info = env_rendering.step(action)\n",
    "    env_rendering.render()\n",
    "    if done:\n",
    "      obs = env_rendering.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env_rendering.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "795px",
    "left": "1545px",
    "right": "20px",
    "top": "125px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
