{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "from ray.air import RunConfig\n",
    "from ray import air, tune\n",
    "\n",
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.integrate as sc_integrate\n",
    "\n",
    "import symbtools as st\n",
    "import sympy as sp\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical system description with SymPy / symbtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model etc. from pickle of flatness analysis notebook\n",
    "with open(\"double_crane_model.pcl\", \"rb\") as pfile:\n",
    "    data = pickle.load(pfile)\n",
    "    locals().update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}p_{1}\\\\p_{2}\\\\p_{3}\\\\q_{1}\\\\q_{2}\\\\\\dot{p}_{1}\\\\\\dot{p}_{2}\\\\\\dot{p}_{3}\\\\\\dot{q}_{1}\\\\\\dot{q}_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   p1],\n",
       "[   p2],\n",
       "[   p3],\n",
       "[   q1],\n",
       "[   q2],\n",
       "[pdot1],\n",
       "[pdot2],\n",
       "[pdot3],\n",
       "[qdot1],\n",
       "[qdot2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}m_{2} \\ddot{p}_{1} - \\frac{\\tau_{3} \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}} - \\frac{\\tau_{4} \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}}\\\\g m_{2} + m_{2} \\ddot{p}_{2} - \\frac{\\tau_{3} \\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}} - \\frac{\\tau_{4} \\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}}\\\\J_{2} \\ddot{p}_{3} + \\frac{s_{2} \\tau_{3} \\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right) \\cos{\\left(p_{3} \\right)}}{\\sqrt{\\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}} - \\frac{s_{2} \\tau_{3} \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right) \\sin{\\left(p_{3} \\right)}}{\\sqrt{\\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}} - \\frac{s_{2} \\tau_{4} \\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right) \\cos{\\left(p_{3} \\right)}}{\\sqrt{\\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}} + \\frac{s_{2} \\tau_{4} \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right) \\sin{\\left(p_{3} \\right)}}{\\sqrt{\\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}}\\\\m_{1} \\ddot{q}_{1} - \\tau_{1} + \\frac{\\tau_{3} \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} - s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(p_{1} - q_{1} - s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}}\\\\m_{3} \\ddot{q}_{2} - \\tau_{2} + \\frac{\\tau_{4} \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)}{\\sqrt{\\left(p_{2} + s_{2} \\sin{\\left(p_{3} \\right)}\\right)^{2} + \\left(- l_{0} + p_{1} - q_{2} + s_{2} \\cos{\\left(p_{3} \\right)}\\right)^{2}}}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                                                                                                                                                                                                                  m2*pddot1 - tau3*(p1 - q1 - s2*cos(p3))/sqrt((p2 - s2*sin(p3))**2 + (p1 - q1 - s2*cos(p3))**2) - tau4*(-l0 + p1 - q2 + s2*cos(p3))/sqrt((p2 + s2*sin(p3))**2 + (-l0 + p1 - q2 + s2*cos(p3))**2)],\n",
       "[                                                                                                                                                                                                                           g*m2 + m2*pddot2 - tau3*(p2 - s2*sin(p3))/sqrt((p2 - s2*sin(p3))**2 + (p1 - q1 - s2*cos(p3))**2) - tau4*(p2 + s2*sin(p3))/sqrt((p2 + s2*sin(p3))**2 + (-l0 + p1 - q2 + s2*cos(p3))**2)],\n",
       "[J2*pddot3 + s2*tau3*(p2 - s2*sin(p3))*cos(p3)/sqrt((p2 - s2*sin(p3))**2 + (p1 - q1 - s2*cos(p3))**2) - s2*tau3*(p1 - q1 - s2*cos(p3))*sin(p3)/sqrt((p2 - s2*sin(p3))**2 + (p1 - q1 - s2*cos(p3))**2) - s2*tau4*(p2 + s2*sin(p3))*cos(p3)/sqrt((p2 + s2*sin(p3))**2 + (-l0 + p1 - q2 + s2*cos(p3))**2) + s2*tau4*(-l0 + p1 - q2 + s2*cos(p3))*sin(p3)/sqrt((p2 + s2*sin(p3))**2 + (-l0 + p1 - q2 + s2*cos(p3))**2)],\n",
       "[                                                                                                                                                                                                                                                                                                            m1*qddot1 - tau1 + tau3*(p1 - q1 - s2*cos(p3))/sqrt((p2 - s2*sin(p3))**2 + (p1 - q1 - s2*cos(p3))**2)],\n",
       "[                                                                                                                                                                                                                                                                                                m3*qddot2 - tau2 + tau4*(-l0 + p1 - q2 + s2*cos(p3))/sqrt((p2 + s2*sin(p3))**2 + (-l0 + p1 - q2 + s2*cos(p3))**2)]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.calc_state_eq(force_recalculation=True)\n",
    "mod.eqns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import inspect\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "#from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "#from gym.utils.renderer import Renderer\n",
    "\n",
    "class StateSpaceModel(gym.Env):\n",
    "    \"\"\" Environment subclass that uses a state space model of the form dx/dt = f(x, u)\n",
    "    to represent the environments dynamics.\n",
    "\n",
    "    Args:\n",
    "        ode\n",
    "        cost\n",
    "        x0\n",
    "        uDim\n",
    "\n",
    "    Attributes:\n",
    "        ode (function): ODE for simulation\n",
    "        cost (function): cost function (returns scalar)\n",
    "        o\n",
    "        o_\n",
    "        oDim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ode, cost, x0, uDim, dt, yRef,\n",
    "                 terminal_cost=-1e4):\n",
    "        self.tt = [0]\n",
    "        self.dt = dt\n",
    "        if callable(x0):\n",
    "            self.x0 = x0  # initial state\n",
    "            x0 = x0()\n",
    "        else:\n",
    "            x0 = list(x0)\n",
    "            self.x0 = x0\n",
    "        self.x = x0  # current state\n",
    "        self.x_ = x0 # previous state x[k-1]\n",
    "        self.yRef = yRef # reference values of flat output\n",
    "        self.xDim = len(x0) # state dimension\n",
    "        self.uDim = uDim # inputs\n",
    "        self.o = self.x - np.array([*self.yRef, *np.zeros(6)])\n",
    "        self.o_ = self.x_ - np.array([*self.yRef, *np.zeros(6)])\n",
    "        self.oDim = len(self.o)  # observation dimensions\n",
    "        self.ode = ode\n",
    "        params = inspect.signature(cost).parameters\n",
    "        cost_args = params.__len__()\n",
    "        if cost_args == 1:\n",
    "            self.cost = lambda o_, u_, o, t, mod: cost(o_)\n",
    "        elif cost_args == 2:\n",
    "            if 'mod' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, mod)\n",
    "            elif 't' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, t)\n",
    "            else:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_)\n",
    "        elif cost_args == 3:\n",
    "            if 'mod' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, mod)\n",
    "            elif 't' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, t)\n",
    "            else:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, o)\n",
    "        elif cost_args == 4:\n",
    "            if 'mod' in params and 't' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, t, mod)\n",
    "            elif 'mod' in params and not 't' in params:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, o, mod)\n",
    "            else:\n",
    "                self.cost = lambda o_, u_, o, t, mod: cost(o_, u_, o, t)\n",
    "        elif cost_args == 5:\n",
    "            self.cost = cost\n",
    "        else:\n",
    "            print('Cost function must to be of the form c(o_, u_, o, t, mod), where mod is numpy/sympy.')\n",
    "            assert(True)\n",
    "            \n",
    "        self.terminated = False\n",
    "        self.terminal_cost = terminal_cost\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.x\n",
    "\n",
    "    def set_ref_values(self):\n",
    "        '''\n",
    "        yRef1 = [0.2, -0.2, 0.1, 0]\n",
    "        yRef2 = [0.3, -0.3, 0.0, 0.2]\n",
    "        yRef3 = [0.4, -0.1, -0.1, -0.1]\n",
    "        yRef4 = [0.5, -0.4, 0.2, 0.1]\n",
    "        yRefs = [yRef1, yRef2, yRef3, yRef4]\n",
    "        \n",
    "        selec_ind = np.random.randint(0,3)\n",
    "        self.yRef = yRefs[selec_ind]\n",
    "        self.yRef = self.yRef + np.random.uniform(-0.05,0.05, 4)\n",
    "\n",
    "        '''\n",
    "        self.yRef = np.random.uniform(-1.0,1.0, 4)\n",
    " \n",
    "    def reset(self):\n",
    "        \"\"\" Resets environment to state x0\n",
    "\n",
    "        Args:\n",
    "            x0 (array, list, callable): initial state\n",
    "\n",
    "        \"\"\"\n",
    "        if callable(self.x0):\n",
    "            x0 = self.x0()\n",
    "        self.x_ = x0\n",
    "        self.x = x0\n",
    "        self.tt = [0]\n",
    "        self.terminated = False\n",
    "        self.set_ref_values()\n",
    "        return np.array(self.x, dtype=np.float32)\n",
    "\n",
    "    def step(self, *args):\n",
    "        \"\"\" Simulates the environment for 1 step of time t.\n",
    "\n",
    "        Args:\n",
    "            dt (int, float): duration of step (not solver step size)\n",
    "            u (array): control/action\n",
    "\n",
    "        Returns:\n",
    "            c (float): cost of state transition\n",
    "\n",
    "        \"\"\"\n",
    "        self.x_ = self.x  # shift state (x[k-1] = x[k])\n",
    "        self.o_ = self.o\n",
    "        if args.__len__()==2:\n",
    "            u = args[0]\n",
    "            dt = args[1]\n",
    "        elif args.__len__() == 1:\n",
    "            u = args[0]\n",
    "            dt = self.dt\n",
    "\n",
    "        # system simulation\n",
    "        sol = solve_ivp(lambda t, x: self.ode(t, x, u, self.dxdt), (0, dt), self.x_, 'RK45')\n",
    "        # todo: only output value of the last timestep\n",
    "        y = list(sol.y[:, -1])  # extract simulation result\n",
    "        self.x = y\n",
    "        self.tt.extend([self.tt[-1] + dt])  # increment simulation time\n",
    "        self.terminated = self.terminate(self.x)\n",
    "        #x_2pi = mapAngles(self.xIsAngle, self.x_)\n",
    "        #x2pi = mapAngles(self.xIsAngle, self.x)\n",
    "        #c = (self.cost(x_2pi, u, x2pi, np) + self.terminal_cost*self.terminated)*dt\n",
    "        t = self.tt[-1]\n",
    "        c = (self.cost(self.o_, u, self.x, t, np) + self.terminal_cost * self.terminated) * dt\n",
    "        \n",
    "        # todo: place difference for desired value with function or smth here\n",
    "        self.o =  np.array(self.x - np.array([*self.yRef, *np.zeros(6)]), dtype=np.float32)\n",
    "        reward = c\n",
    "        return self.o, reward, self.terminated, {\"info\": False}\n",
    "    \n",
    "    @abstractmethod\n",
    "    def terminate(self, *args):\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "class GantryCraneEnv(StateSpaceModel):\n",
    "\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, cost, x0, dt, mod, yRef):\n",
    "        super(GantryCraneEnv, self).__init__(self.ode, cost, x0, 4, dt, yRef)\n",
    "        \n",
    "        self.spec = gym.envs.registration.EnvSpec(id='GantryCraneEnv-v0', max_episode_steps=1000)\n",
    "        \n",
    "        F1, F2, F3, F4 = sp.symbols('F1 F2 F3 F4')\n",
    "\n",
    "        params = sp.symbols('s2, m1, m2, m3, J2, l0, g')\n",
    "        st.make_global(params)\n",
    "        params_values = list(dict(J2=0.004553475, g=9.81, l0=0.3, m1=0.45, m2=0.557, m3=0.45, s2=0.15).items())\n",
    "        \n",
    "        states_dot = mod.f + mod.g * sp.Matrix([F1, F2, F3, F4]) ##:\n",
    "        states_dot_wo_params = states_dot.subs(params_values)\n",
    "        self.dxdt = st.expr_to_func([*mod.xx, F1, F2, F3, F4], states_dot_wo_params)\n",
    "\n",
    "        # define observation space\n",
    "        high_obs = np.array(\n",
    "            [\n",
    "                100.0,\n",
    "                100.0,\n",
    "                100.0,\n",
    "                100.0,\n",
    "                100.0,\n",
    "                100.0, \n",
    "                100.0,\n",
    "                100.0,\n",
    "                100.0, \n",
    "                100.0\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high_obs, high_obs, dtype=np.float32)\n",
    "        \n",
    "        # define action space\n",
    "        high_act = np.array(\n",
    "            [ \n",
    "                5.0,\n",
    "                5.0,\n",
    "                5.0,\n",
    "                5.0\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = spaces.Box(-high_act, high_act, dtype=np.float32)\n",
    "    \n",
    "    def ode(self, t, x, u, dxdt):\n",
    "        u1, u2, u3, u4 = u\n",
    "        states_dot = dxdt(*x, u1, u2, u3 - 2.732085, u4 - 2.732085)\n",
    "        return np.array(states_dot, dtype=np.float32)\n",
    "    \n",
    "    def terminate(self, x):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10 = x\n",
    "        if abs(x3) > 0.5 or abs(x4) > 10 or abs(x5) > 10:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function, Initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for reference learning look on:\n",
    "# https://docs.ray.io/en/latest/rllib/rllib-offline.html\n",
    "# https://docs.ray.io/en/latest/rllib/package_ref/offline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}p_{1} & p_{2} & p_{3} & q_{1} & q_{2} & \\dot{p}_{1} & \\dot{p}_{2} & \\dot{p}_{3} & \\dot{q}_{1} & \\dot{q}_{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[p1, p2, p3, q1, q2, pdot1, pdot2, pdot3, qdot1, qdot2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.xx.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the incremental cost\n",
    "def c_k(o, u):\n",
    "    e1, e2, e3, e4, x5, x6, x7, x8, x9, x10 = o\n",
    "    u1, u2, u3, u4 = u\n",
    "    \n",
    "    c = -10*e1**2 -10*e2**2 - 10*e3**2 - 10*e4**2 #- x10**2 \n",
    "    \n",
    "    if abs(e3) < 0.02:\n",
    "        c = c + 1\n",
    "    \n",
    "    if abs(e2) < 0.02:\n",
    "        c = c + 1\n",
    "\n",
    "    if abs(e1) < 0.02:\n",
    "        c = c + 1\n",
    "        \n",
    "    return c\n",
    "\n",
    "# define the function, that represents the initial value distribution p(x_0)\n",
    "def p_x0():\n",
    "    x0 = [\n",
    "            1.49999995e-01 + np.random.uniform(-0.001,0.001), \n",
    "            -4.00000004e-01 + np.random.uniform(-0.001,0.001), \n",
    "             4.43002187e-08 + np.random.uniform(-0.001,0.001), \n",
    "            0.00000000e+00 + np.random.uniform(-0.001,0.001),\n",
    "            0.00000000e+00 + np.random.uniform(-0.001,0.001),\n",
    "            np.random.uniform(-0.001,0.001),\n",
    "            np.random.uniform(-0.001,0.001),\n",
    "            np.random.uniform(-0.001,0.001),\n",
    "            np.random.uniform(-0.001,0.001),\n",
    "            np.random.uniform(-0.001,0.001)\n",
    "    ]\n",
    "    return x0\n",
    "\n",
    "t = 10 # time of an episode\n",
    "dt = 0.01 # time step-size\n",
    "yRef0 = [0.2, -0.2, 0.1, 0]\n",
    "\n",
    "custom_local_dir=\"/home/kwrede/ray_results/custom_logdir\"\n",
    "learning_iterations = int(100) # define training iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment without renderings for training\n",
    "env = GantryCraneEnv(c_k, p_x0, dt, mod, yRef0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.], (10,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(env.action_space.sample())\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure RL algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 15:25:09,448\tINFO worker.py:1518 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.10</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.8.10', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '172.25.59.7', 'raylet_ip_address': '172.25.59.7', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-18_15-25-07_512236_20804/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-18_15-25-07_512236_20804/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-10-18_15-25-07_512236_20804', 'metrics_export_port': 65171, 'gcs_address': '172.25.59.7:57689', 'address': '172.25.59.7:57689', 'dashboard_agent_listen_port': 52365, 'node_id': 'd999077e60c37c245ac39290a0fe2913d6366d805bbd3fc098367890'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(dashboard_host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    return GantryCraneEnv(**env_config) # return an env instanceregister_env(\"my_env\", env_creator)\n",
    "\n",
    "register_env(\"GantryCraneCustom\", env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 15:25:10,842\tWARNING ppo.py:350 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=8 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 500.\n",
      "2022-10-18 15:25:10,844\tINFO ppo.py:378 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-10-18 15:25:10,846\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21290)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21289)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21291)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21295)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21300)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21297)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21293)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=21302)\u001b[0m \u001b[22;0t\u001b]0;IPython: SA-Wrede/reinforcement_learning_control\u0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 15:25:19,706\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"num_workers\"] = 8\n",
    "config[\"framework\"] = \"torch\"\n",
    "config[\"env_config\"] = {\"cost\":c_k, \"x0\":p_x0, \"dt\":dt, \"mod\":mod, \"yRef\":yRef0}\n",
    "config[\"lambda\"] = 0.95\n",
    "config[\"clip_param\"] = 0.2\n",
    "config[\"kl_coeff\"] = 1.0\n",
    "config[\"lr\"] = 1e-4\n",
    "\n",
    "algo = ppo.PPO(config=config, env=\"GantryCraneCustom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                               | 3/1000 [00:24<2:13:03,  8.01s/it]Exception ignored in: <generator object tqdm.__iter__ at 0x7f99e4609740>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/std.py\", line 1210, in __iter__\n",
      "    self.close()\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/std.py\", line 1316, in close\n",
      "    self.display(pos=0)\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/std.py\", line 1509, in display\n",
      "    self.sp(self.__str__() if msg is None else msg)\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/std.py\", line 350, in print_status\n",
      "    fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/std.py\", line 344, in fp_write\n",
      "    fp_flush()\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/tqdm/utils.py\", line 145, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/kwrede/.local/lib/python3.8/site-packages/ipykernel/iostream.py\", line 483, in flush\n",
      "    if not evt.wait(self.flush_timeout):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 558, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 306, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(learning_iterations)):\n\u001b[1;32m      2\u001b[0m    \u001b[38;5;66;03m# Perform one iteration of training the policy with PPO\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m    result \u001b[38;5;241m=\u001b[39m \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m    \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      6\u001b[0m        checkpoint \u001b[38;5;241m=\u001b[39m algo\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable/trainable.py:347\u001b[0m, in \u001b[0;36mTrainable.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warmup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n\u001b[1;32m    346\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 347\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep() needs to return a dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# We do not modify internal state nor update this result if duplicate.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:661\u001b[0m, in \u001b[0;36mAlgorithm.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    653\u001b[0m     (\n\u001b[1;32m    654\u001b[0m         results,\n\u001b[1;32m    655\u001b[0m         train_iter_ctx,\n\u001b[1;32m    656\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     results, train_iter_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_training_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_this_iter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_parallel_to_training\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py:2373\u001b[0m, in \u001b[0;36mAlgorithm._run_one_training_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001b[1;32m   2372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 2373\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2375\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_exec_impl)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/algorithms/ppo/ppo.py:420\u001b[0m, in \u001b[0;36mPPO.training_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    418\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m train_one_step(\u001b[38;5;28mself\u001b[39m, train_batch)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     train_results \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_gpu_train_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m global_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_counters[NUM_AGENT_STEPS_SAMPLED],\n\u001b[1;32m    424\u001b[0m }\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Update weights - after learning on the local worker - on all remote\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py:176\u001b[0m, in \u001b[0;36mmulti_gpu_train_one_step\u001b[0;34m(algorithm, train_batch)\u001b[0m\n\u001b[1;32m    171\u001b[0m         permutation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_batches)\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    173\u001b[0m             \u001b[38;5;66;03m# Learn on the pre-loaded data in the buffer.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m             \u001b[38;5;66;03m# Note: For minibatch SGD, the data is an offset into\u001b[39;00m\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;66;03m# the pre-loaded entire train batch.\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m             results \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_loaded_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpermutation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mper_device_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m             learner_info_builder\u001b[38;5;241m.\u001b[39madd_learn_on_batch_results(results, policy_id)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Tower reduce and finalize results.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/dynamic_tf_policy_v2.py:1029\u001b[0m, in \u001b[0;36mDynamicTFPolicyV2.learn_on_loaded_batch\u001b[0;34m(self, offset, buffer_index)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m         sliced_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_single_cpu_batch\u001b[38;5;241m.\u001b[39mslice(\n\u001b[1;32m   1027\u001b[0m             start\u001b[38;5;241m=\u001b[39moffset, end\u001b[38;5;241m=\u001b[39moffset \u001b[38;5;241m+\u001b[39m batch_size\n\u001b[1;32m   1028\u001b[0m         )\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msliced_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_gpu_tower_stacks[buffer_index]\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session(), offset\n\u001b[1;32m   1033\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:436\u001b[0m, in \u001b[0;36mTFPolicy.learn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_learn_on_batch(\n\u001b[1;32m    432\u001b[0m     policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, train_batch\u001b[38;5;241m=\u001b[39mpostprocessed_batch, result\u001b[38;5;241m=\u001b[39mlearn_stats\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m fetches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_learn_on_batch(builder, postprocessed_batch)\n\u001b[0;32m--> 436\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m stats\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    438\u001b[0m     {\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m: learn_stats,\n\u001b[1;32m    440\u001b[0m         NUM_AGENT_STEPS_TRAINED: postprocessed_batch\u001b[38;5;241m.\u001b[39mcount,\n\u001b[1;32m    441\u001b[0m     }\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stats\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py:42\u001b[0m, in \u001b[0;36m_TFRunBuilder.get\u001b[0;34m(self, to_fetch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m \u001b[43m_run_timeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebug_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTF_TIMELINE_DIR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     50\u001b[0m         logger\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError fetching: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, feed_dict=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     52\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_dict\n\u001b[1;32m     53\u001b[0m             )\n\u001b[1;32m     54\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/utils/tf_run_builder.py:102\u001b[0m, in \u001b[0;36m_run_timeline\u001b[0;34m(sess, ops, debug_name, feed_dict, timeline_dir)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m log_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_timeline\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     98\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting TF run without tracing. To dump TF timeline traces \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto disk, set the TF_TIMELINE_DIR environment variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[0;32m--> 102\u001b[0m     fetches \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fetches\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    964\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    970\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1190\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1190\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1193\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1370\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1370\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1377\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1376\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1379\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1360\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1358\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1453\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1452\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1453\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(learning_iterations)):\n",
    "   # Perform one iteration of training the policy with PPO\n",
    "   result = algo.train()\n",
    "\n",
    "   if i % 100 == 0:\n",
    "       checkpoint = algo.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tuner' object has no attribute 'compute_single_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m act_mean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_single_action\u001b[49m(obs)\n\u001b[1;32m     18\u001b[0m     act_mean \u001b[38;5;241m=\u001b[39m act_mean \u001b[38;5;241m+\u001b[39m action\n\u001b[1;32m     19\u001b[0m action \u001b[38;5;241m=\u001b[39m act_mean\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tuner' object has no attribute 'compute_single_action'"
     ]
    }
   ],
   "source": [
    "u1_for_plots = []\n",
    "u2_for_plots = []\n",
    "u3_for_plots = []\n",
    "u4_for_plots = []\n",
    "e1_for_plots = []\n",
    "e2_for_plots = []\n",
    "e3_for_plots = []\n",
    "e4_for_plots = []\n",
    "x5_for_plots = []\n",
    "\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    # inference smoothing!\n",
    "    act_mean = [0, 0, 0, 0]\n",
    "    for i in range(0, 4):\n",
    "        action = algo.compute_single_action(obs)\n",
    "        act_mean = act_mean + action\n",
    "    action = act_mean/4\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    u1_for_plots = np.append(u1_for_plots, action[0])\n",
    "    u2_for_plots = np.append(u2_for_plots, action[1])\n",
    "    u3_for_plots = np.append(u3_for_plots, action[2])\n",
    "    u4_for_plots = np.append(u4_for_plots, action[3])\n",
    "    e1_for_plots = np.append(e1_for_plots, obs[0])\n",
    "    e2_for_plots = np.append(e2_for_plots, obs[1])\n",
    "    e3_for_plots = np.append(e3_for_plots, obs[2])\n",
    "    e4_for_plots = np.append(e4_for_plots, obs[3])\n",
    "    x5_for_plots = np.append(x5_for_plots, obs[4])\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "      break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steady state error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"e1 =\", e1_for_plots[-1])\n",
    "print(\"e2 =\", e2_for_plots[-2])\n",
    "print(\"e3 =\", e3_for_plots[-3])\n",
    "print(\"e4 =\", e4_for_plots[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(e1_for_plots, label=r\"$e_1$\")\n",
    "plt.plot(e2_for_plots, label=r\"$e_2$\")\n",
    "plt.plot(e3_for_plots, label=r\"$e_3$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e4_for_plots, label=r\"$e_{q_1}$\")\n",
    "plt.plot(x5_for_plots, label=r\"$q_2$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(u1_for_plots, label=r\"$u_1$\")\n",
    "plt.plot(u2_for_plots, label=r\"$u_2$\")\n",
    "plt.plot(u3_for_plots, label=r\"$u_3$\")\n",
    "plt.plot(u4_for_plots, label=r\"$u_4$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "795px",
    "left": "1545px",
    "right": "20px",
    "top": "125px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
