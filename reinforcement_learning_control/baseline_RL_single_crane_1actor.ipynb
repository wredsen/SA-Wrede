{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.integrate as sc_integrate\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import A2C, DQN\n",
    "\n",
    "# using wredsen's symbtools fork (https://github.com/wredsen/symbtools @ DAE_statefeedback), assuming repos SA-Wrede and symbtools share the same parent directory\n",
    "sys.path.append('../../symbtools/')\n",
    "import symbtools as st\n",
    "import sympy as sp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical system description with SymPy / symbtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Np = 1 # number of passive coordinates (p = phi1)\n",
    "Nq = 1 # number of actuated coordinates (q = x0)\n",
    "n = Np + Nq\n",
    "pp = st.symb_vector(\"p1:{0}\".format(Np+1))\n",
    "qq = st.symb_vector(\"q1:{0}\".format(Nq+1))\n",
    "\n",
    "# all coordinates and their derivatives\n",
    "ttheta = st.row_stack(pp, qq) ##:T\n",
    "tthetad = st.time_deriv(ttheta, ttheta) ##:T\n",
    "tthetadd = st.time_deriv(ttheta, ttheta, order=2) ##:T\n",
    "st.make_global(ttheta, tthetad)\n",
    "\n",
    "F = sp.Symbol('F')\n",
    "\n",
    "params = sp.symbols('m0, m1, l1, g, d0, d1')\n",
    "st.make_global(params)\n",
    "params_values = [(m0, 1.0), (m1, 0.1), (l1, 0.5), (g, 9.81),\n",
    "                 (d0, 0.01), (d1, 0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model etc. from pickle of flatness analysis notebook\n",
    "with open(\"../single_crane_notebooks/model_single_crane.pcl\", \"rb\") as pfile:\n",
    "    mod = pickle.load(pfile) # mod instead of data\n",
    "    #locals().update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}p_{1}\\\\q_{1}\\\\\\dot{p}_{1}\\\\\\dot{q}_{1}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   p1],\n",
       "[   q1],\n",
       "[pdot1],\n",
       "[qdot1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}d_{0} \\dot{p}_{1} + g l_{1} m_{1} \\sin{\\left(p_{1} \\right)} + l_{1}^{2} m_{1} \\ddot{p}_{1} - l_{1} m_{1} \\ddot{q}_{1} \\cos{\\left(p_{1} \\right)}\\\\- Fcomp + 1.0 m_{0} \\ddot{q}_{1} + m_{1} \\left(- l_{1} \\ddot{p}_{1} \\cos{\\left(p_{1} \\right)} + l_{1} \\dot{p}_{1}^{2} \\sin{\\left(p_{1} \\right)} + \\ddot{q}_{1}\\right)\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[            d0*pdot1 + g*l1*m1*sin(p1) + l1**2*m1*pddot1 - l1*m1*qddot1*cos(p1)],\n",
       "[-Fcomp + 1.0*m0*qddot1 + m1*(-l1*pddot1*cos(p1) + l1*pdot1**2*sin(p1) + qddot1)]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.calc_state_eq(force_recalculation=True)\n",
    "mod.eqns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "states_dot = mod.f + mod.g * F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dot_wo_params = states_dot.subs(params_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_dot_func = st.expr_to_func([*mod.xx, F], states_dot_wo_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions used for classic control environments.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional, SupportsFloat, Tuple\n",
    "\n",
    "\n",
    "def verify_number_and_cast(x: SupportsFloat) -> float:\n",
    "    \"\"\"Verify parameter is a single number and cast to a float.\"\"\"\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        raise ValueError(f\"An option ({x}) could not be converted to a float.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def maybe_parse_reset_bounds(\n",
    "    options: Optional[dict], default_low: float, default_high: float\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    This function can be called during a reset() to customize the sampling\n",
    "    ranges for setting the initial state distributions.\n",
    "\n",
    "    Args:\n",
    "      options: Options passed in to reset().\n",
    "      default_low: Default lower limit to use, if none specified in options.\n",
    "      default_high: Default upper limit to use, if none specified in options.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of the lower and upper limits.\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        return default_low, default_high\n",
    "\n",
    "    low = options.get(\"low\") if \"low\" in options else default_low\n",
    "    high = options.get(\"high\") if \"high\" in options else default_high\n",
    "\n",
    "    # We expect only numerical inputs.\n",
    "    low = verify_number_and_cast(low)\n",
    "    high = verify_number_and_cast(high)\n",
    "    if low > high:\n",
    "        raise ValueError(\n",
    "            f\"Lower bound ({low}) must be lower than higher bound ({high}).\"\n",
    "        )\n",
    "\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A utility class to collect render frames from a function that computes a single frame.\"\"\"\n",
    "from typing import Any, Callable, List, Optional, Set\n",
    "\n",
    "# list of modes with which render function returns None\n",
    "NO_RETURNS_RENDER = {\"human\"}\n",
    "\n",
    "# list of modes with which render returns just a single frame of the current state\n",
    "SINGLE_RENDER = {\"single_rgb_array\", \"single_depth_array\", \"single_state_pixels\"}\n",
    "\n",
    "\n",
    "class Renderer:\n",
    "    \"\"\"This class serves to easily integrate collection of renders for environments that can computes a single render.\n",
    "\n",
    "    To use this function:\n",
    "    - instantiate this class with the mode and the function that computes a single frame\n",
    "    - call render_step method each time the frame should be saved in the list\n",
    "      (usually at the end of the step and reset methods)\n",
    "    - call get_renders whenever you want to retrieve renders\n",
    "      (usually in the render method)\n",
    "    - call reset to clean the render list\n",
    "      (usually in the reset method of the environment)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: Optional[str],\n",
    "        render: Callable[[str], Any],\n",
    "        no_returns_render: Optional[Set[str]] = None,\n",
    "        single_render: Optional[Set[str]] = None,\n",
    "    ):\n",
    "        \"\"\"Instantiates a Renderer object.\n",
    "\n",
    "        Args:\n",
    "            mode (Optional[str]): Way to render\n",
    "            render (Callable[[str], Any]): Function that receives the mode and computes a single frame\n",
    "            no_returns_render (Optional[Set[str]]): Set of render modes that don't return any value.\n",
    "                The default value is the set {\"human\"}.\n",
    "            single_render (Optional[Set[str]]): Set of render modes that should return a single frame.\n",
    "                The default value is the set {\"single_rgb_array\", \"single_depth_array\", \"single_state_pixels\"}.\n",
    "        \"\"\"\n",
    "        if no_returns_render is None:\n",
    "            no_returns_render = NO_RETURNS_RENDER\n",
    "        if single_render is None:\n",
    "            single_render = SINGLE_RENDER\n",
    "\n",
    "        self.no_returns_render = no_returns_render\n",
    "        self.single_render = single_render\n",
    "        self.mode = mode\n",
    "        self.render = render\n",
    "        self.render_list = []\n",
    "\n",
    "    def render_step(self) -> None:\n",
    "        \"\"\"Computes a frame and save it to the render collection list.\n",
    "\n",
    "        This method should be usually called inside environment's step and reset method.\n",
    "        \"\"\"\n",
    "        if self.mode is not None and self.mode not in self.single_render:\n",
    "            render_return = self.render(self.mode)\n",
    "            if self.mode not in self.no_returns_render:\n",
    "                self.render_list.append(render_return)\n",
    "\n",
    "    def get_renders(self) -> Optional[List]:\n",
    "        \"\"\"Pops all the frames from the render collection list.\n",
    "\n",
    "        This method should be usually called in the environment's render method to retrieve the frames collected till this time step.\n",
    "        \"\"\"\n",
    "        if self.mode in self.single_render:\n",
    "            return self.render(self.mode)\n",
    "        elif self.mode is not None and self.mode not in self.no_returns_render:\n",
    "            renders = self.render_list\n",
    "            self.render_list = []\n",
    "            return renders\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the render collection list.\n",
    "\n",
    "        This method should be usually called inside environment's reset method.\n",
    "        \"\"\"\n",
    "        self.render_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym environment for crane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "#from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "#from gym.utils.renderer import Renderer\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ### Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ### Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ### Rewards\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "    ### Starting State\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ### Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('CartPole-v1')\n",
    "    ```\n",
    "\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\", \"single_rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "    \n",
    "        # geometrics for rendering\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        \n",
    "        # magnitude for discrete force applied\n",
    "        self.force_mag = 0.10\n",
    "        \n",
    "        #simulation step width\n",
    "        self.deltaT = 0.02  # seconds between state updates\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        high_act = np.array(\n",
    "            [ \n",
    "                1.0\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.action_space = spaces.Box(-high_act, high_act, dtype=np.float32)\n",
    "        \n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high_obs = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(-high_obs, high_obs, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.renderer = Renderer(self.render_mode, self._render)\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        p1, q1, p1_dot, q1_dot = self.state\n",
    "        force = self.force_mag * action\n",
    "        \n",
    "        states_dot_now = states_dot_func(*self.state, force)\n",
    "        p1, q1, p1_dot, q1_dot = self.state + self.deltaT * states_dot_now\n",
    "        \n",
    "        self.state = (p1, q1, p1_dot, q1_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            q1 < -self.x_threshold\n",
    "            or q1 > self.x_threshold\n",
    "            or p1 < -self.theta_threshold_radians\n",
    "            or p1 > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        self.renderer.render_step()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, {\"info\": False}#False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        #super().reset()(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = np.random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "        self.renderer.reset()\n",
    "        self.renderer.render_step()\n",
    "        return np.array(self.state, dtype=np.float32)#, {}\n",
    "\n",
    "    def render(self):\n",
    "        return self.renderer.get_renders()\n",
    "\n",
    "    def _render(self, mode=\"human\"):\n",
    "        assert mode in self.metadata[\"render_modes\"]\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode in {\"rgb_array\", \"single_rgb_array\"}\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif mode in {\"rgb_array\", \"single_rgb_array\"}:\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment without renderings for training\n",
    "env = CartPoleEnv()\n",
    "# environment with renderings for validating\n",
    "env_rendering = CartPoleEnv(render_mode = \"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(env.action_space.sample())\n",
    "env.observation_space\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 16:36:34.146913: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-02 16:36:34.149797: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/kwrede/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-09-02 16:36:34.149804: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./a2c_cartpole_tensorboard/A2C_22\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.0156  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 2.73     |\n",
      "|    std                | 0.966    |\n",
      "|    value_loss         | 7.44     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -0.000504 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.29      |\n",
      "|    std                | 0.962     |\n",
      "|    value_loss         | 6.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1.17e+03  |\n",
      "|    ep_rew_mean        | 1.17e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 646       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.39     |\n",
      "|    explained_variance | -0.000209 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 2.84      |\n",
      "|    std                | 0.969     |\n",
      "|    value_loss         | 5.39      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 808      |\n",
      "|    ep_rew_mean        | 808      |\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -9.3e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.96     |\n",
      "|    std                | 0.964    |\n",
      "|    value_loss         | 4.72     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 606      |\n",
      "|    ep_rew_mean        | 606      |\n",
      "| time/                 |          |\n",
      "|    fps                | 781      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -0.00576 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 2.58     |\n",
      "|    std                | 0.966    |\n",
      "|    value_loss         | 4.15     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 562       |\n",
      "|    ep_rew_mean        | 562       |\n",
      "| time/                 |           |\n",
      "|    fps                | 823       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -0.000164 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 1.71      |\n",
      "|    std                | 0.957     |\n",
      "|    value_loss         | 3.55      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 531      |\n",
      "|    ep_rew_mean        | 531      |\n",
      "| time/                 |          |\n",
      "|    fps                | 864      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.8      |\n",
      "|    std                | 0.944    |\n",
      "|    value_loss         | 3.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 495      |\n",
      "|    ep_rew_mean        | 495      |\n",
      "| time/                 |          |\n",
      "|    fps                | 899      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | -0.00437 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.43     |\n",
      "|    std                | 0.941    |\n",
      "|    value_loss         | 2.56     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 486       |\n",
      "|    ep_rew_mean        | 486       |\n",
      "| time/                 |           |\n",
      "|    fps                | 931       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.35     |\n",
      "|    explained_variance | -0.000286 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 2.94      |\n",
      "|    std                | 0.936     |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 479       |\n",
      "|    ep_rew_mean        | 479       |\n",
      "| time/                 |           |\n",
      "|    fps                | 957       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | -3.15e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 1.62      |\n",
      "|    std                | 0.945     |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 474      |\n",
      "|    ep_rew_mean        | 474      |\n",
      "| time/                 |          |\n",
      "|    fps                | 981      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.37     |\n",
      "|    std                | 0.945    |\n",
      "|    value_loss         | 1.34     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 461       |\n",
      "|    ep_rew_mean        | 461       |\n",
      "| time/                 |           |\n",
      "|    fps                | 994       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | -0.000418 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 2.22      |\n",
      "|    std                | 0.952     |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 456       |\n",
      "|    ep_rew_mean        | 456       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1011      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -8.79e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 1.31      |\n",
      "|    std                | 0.96      |\n",
      "|    value_loss         | 0.757     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 452       |\n",
      "|    ep_rew_mean        | 452       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1028      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.41e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.988     |\n",
      "|    std                | 0.965     |\n",
      "|    value_loss         | 0.524     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 450      |\n",
      "|    ep_rew_mean        | 450      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1043     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.858    |\n",
      "|    std                | 0.965    |\n",
      "|    value_loss         | 0.333    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | 447      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1056     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.538    |\n",
      "|    std                | 0.963    |\n",
      "|    value_loss         | 0.187    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 445       |\n",
      "|    ep_rew_mean        | 445       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1069      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | -0.000388 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.314     |\n",
      "|    std                | 0.955     |\n",
      "|    value_loss         | 0.0841    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 446       |\n",
      "|    ep_rew_mean        | 446       |\n",
      "| time/                 |           |\n",
      "|    fps                | 1081      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.37     |\n",
      "|    explained_variance | -0.000426 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.155     |\n",
      "|    std                | 0.954     |\n",
      "|    value_loss         | 0.0207    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 443      |\n",
      "|    ep_rew_mean        | 443      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1090     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.0259   |\n",
      "|    std                | 0.949    |\n",
      "|    value_loss         | 0.000488 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 440      |\n",
      "|    ep_rew_mean        | 440      |\n",
      "| time/                 |          |\n",
      "|    fps                | 1099     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.37    |\n",
      "|    explained_variance | 5.26e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.000864 |\n",
      "|    std                | 0.948    |\n",
      "|    value_loss         | 9.96e-07 |\n",
      "------------------------------------\n",
      "CPU times: user 8.84 s, sys: 106 ms, total: 8.95 s\n",
      "Wall time: 9.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f655b126a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Learning!\n",
    "model = A2C('MlpPolicy', env, verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model with rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4732:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5220:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2642:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "obs = env_rendering.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, info = env_rendering.step(action)\n",
    "    env_rendering.render()\n",
    "    if done:\n",
    "      obs = env_rendering.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env_rendering.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "795px",
    "left": "1545px",
    "right": "20px",
    "top": "125px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
